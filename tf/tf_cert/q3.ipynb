{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABOUT THE DATASET\n",
    "#\n",
    "# The dataset contains traffic sign boards from the streets captured into\n",
    "# image files. There are 43 unique classes in total. The images are of shape\n",
    "# (30,30,3).\n",
    "# ==============================================================================\n",
    "#\n",
    "# INSTRUCTIONS\n",
    "#\n",
    "# We have already divided the data for training and validation.\n",
    "#\n",
    "# Complete the code in following functions:\n",
    "# 1. preprocess()\n",
    "# 2. solution_model()\n",
    "#\n",
    "# Your code will fail to be graded if the following criteria are not met:\n",
    "# 1. The input shape of your model must be (30,30,3), because the testing\n",
    "#    infrastructure expects inputs according to this specification.\n",
    "# 2. The last layer of your model must be a Dense layer with 43 neurons\n",
    "#    activated by softmax since this dataset has 43 classes.\n",
    "#\n",
    "# HINT: Your neural network must have a validation accuracy of approximately\n",
    "# 0.95 or above on the normalized validation dataset for top marks.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import zipfile\n",
    " \n",
    "import tensorflow as tf\n",
    " \n",
    "# This function downloads and extracts the dataset to the directory that\n",
    "# contains this file.\n",
    "# DO NOT CHANGE THIS CODE\n",
    "# (unless you need to change https to http)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "def download_and_extract_data():\n",
    "   url = 'https://storage.googleapis.com/download.tensorflow.org/data/certificate/germantrafficsigns.zip'\n",
    "   wget.download(url)\n",
    "   #urllib.request.urlretrieve(url, 'germantrafficsigns.zip')\n",
    "   with zipfile.ZipFile('germantrafficsigns.zip', 'r') as zip_ref:\n",
    "       zip_ref.extractall()\n",
    "down_file = download_and_extract_data() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "12\n"
     ]
    }
   ],
   "source": [
    " # COMPLETE THE CODE IN THIS FUNCTION\n",
    "def preprocess(image, label):\n",
    "   # NORMALIZE YOUR IMAGES HERE (HINT: Rescale by 1/.255)\n",
    "   image = image /255\n",
    "   labelZip = label\n",
    "   return image, labelZip\n",
    " \n",
    " \n",
    "# This function loads the data, normalizes and resizes the images, splits it into\n",
    "# train and validation sets, defines the model, compiles it and finally\n",
    "# trains the model. The trained model is returned from this function.\n",
    "import os\n",
    "base_dir = \"./\"\n",
    "train_img = os.path.join(base_dir, 'train')\n",
    "val_img = os.path.join(base_dir, 'validation')\n",
    "\n",
    "train_fname = os.listdir(train_img)\n",
    "val_fname = os.listdir(val_img)\n",
    "print(len(train_fname))\n",
    "print(len(val_fname))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "# COMPLETE THE CODE IN THIS FUNCTION.\n",
    "def solution_model():\n",
    "   # Downloads and extracts the dataset to the directory that\n",
    "   # contains this file.\n",
    "   \n",
    "   \n",
    "   # download_and_extract_data()\n",
    " \n",
    "   BATCH_SIZE = 32\n",
    "   IMG_SIZE = 30\n",
    " \n",
    "   # The following code reads the training and validation data from their\n",
    "   # respective directories, resizes them into the specified image size\n",
    "   # and splits them into batches. You must fill in the image_size\n",
    "   # argument for both training and validation data.\n",
    "   # HINT: Image size is a tuple\n",
    "   train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "       directory='train/',\n",
    "       label_mode='categorical',\n",
    "       image_size=  IMG_SIZE\n",
    "       , batch_size = BATCH_SIZE)\n",
    " \n",
    "   val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "       directory='validation/',\n",
    "       label_mode='categorical',\n",
    "       image_size=  IMG_SIZE\n",
    "       , batch_size = BATCH_SIZE)\n",
    " \n",
    "   # Normalizes train and validation datasets using the\n",
    "   # preprocess() function.\n",
    "   # Also makes other calls, as evident from the code, to prepare them for\n",
    "   # training.\n",
    "   # Do not batch or resize the images in the dataset here since it's already\n",
    "   # been done previously.\n",
    " \n",
    "   train_ds = train_ds.map(\n",
    "       preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(\n",
    "       tf.data.experimental.AUTOTUNE)\n",
    "   val_ds = val_ds.map(\n",
    "       preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "   \n",
    "   return train_ds, val_ds\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 64)        1792      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12544)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               6423040   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 43)                22059     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,446,891\n",
      "Trainable params: 6,446,891\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Code to define the model\n",
    "model = tf.keras.models.Sequential([\n",
    "    # ADD LAYERS OF THE MODEL HERE\n",
    "\n",
    "    # If you don't adhere to the instructions in the following comments,\n",
    "    # tests will fail to grade your model:\n",
    "    # The input layer of your model must have an input shape of\n",
    "    # (30,30,3).\n",
    "    # Make sure your last layer has 43 neurons activated by softmax.\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(30, 30, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(43, activation='softmax')\n",
    "\n",
    "])\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 31367 files belonging to 43 classes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'size' must be a 1-D Tensor of 2 elements: new_height, new_width",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Code to compile and train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_ds, val_ds \u001b[39m=\u001b[39m solution_model()\n\u001b[0;32m      3\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m                 loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m                 metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      8\u001b[0m model\u001b[39m.\u001b[39mfit(train_ds, epochs \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 17\u001b[0m, in \u001b[0;36msolution_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m IMG_SIZE \u001b[39m=\u001b[39m \u001b[39m30\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[39m# The following code reads the training and validation data from their\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m# respective directories, resizes them into the specified image size\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# and splits them into batches. You must fill in the image_size\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m# argument for both training and validation data.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m# HINT: Image size is a tuple\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m train_ds \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mpreprocessing\u001b[39m.\u001b[39;49mimage_dataset_from_directory(\n\u001b[0;32m     18\u001b[0m     directory\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain/\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     19\u001b[0m     label_mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcategorical\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     20\u001b[0m     image_size\u001b[39m=\u001b[39;49m  IMG_SIZE\n\u001b[0;32m     21\u001b[0m     , batch_size \u001b[39m=\u001b[39;49m BATCH_SIZE)\n\u001b[0;32m     23\u001b[0m val_ds \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mpreprocessing\u001b[39m.\u001b[39mimage_dataset_from_directory(\n\u001b[0;32m     24\u001b[0m     directory\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalidation/\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     25\u001b[0m     label_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     26\u001b[0m     image_size\u001b[39m=\u001b[39m  IMG_SIZE\n\u001b[0;32m     27\u001b[0m     , batch_size \u001b[39m=\u001b[39m BATCH_SIZE)\n\u001b[0;32m     29\u001b[0m \u001b[39m# Normalizes train and validation datasets using the\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m# preprocess() function.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39m# Also makes other calls, as evident from the code, to prepare them for\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39m# training.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39m# Do not batch or resize the images in the dataset here since it's already\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39m# been done previously.\u001b[39;00m\n",
      "File \u001b[1;32md:\\ohguk\\py3.8\\lib\\site-packages\\keras\\utils\\image_dataset.py:212\u001b[0m, in \u001b[0;36mimage_dataset_from_directory\u001b[1;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m image_paths:\n\u001b[0;32m    209\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNo images found in directory \u001b[39m\u001b[39m{\u001b[39;00mdirectory\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    210\u001b[0m                    \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAllowed formats: \u001b[39m\u001b[39m{\u001b[39;00mALLOWLIST_FORMATS\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 212\u001b[0m dataset \u001b[39m=\u001b[39m paths_and_labels_to_dataset(\n\u001b[0;32m    213\u001b[0m     image_paths\u001b[39m=\u001b[39;49mimage_paths,\n\u001b[0;32m    214\u001b[0m     image_size\u001b[39m=\u001b[39;49mimage_size,\n\u001b[0;32m    215\u001b[0m     num_channels\u001b[39m=\u001b[39;49mnum_channels,\n\u001b[0;32m    216\u001b[0m     labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m    217\u001b[0m     label_mode\u001b[39m=\u001b[39;49mlabel_mode,\n\u001b[0;32m    218\u001b[0m     num_classes\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(class_names),\n\u001b[0;32m    219\u001b[0m     interpolation\u001b[39m=\u001b[39;49minterpolation,\n\u001b[0;32m    220\u001b[0m     crop_to_aspect_ratio\u001b[39m=\u001b[39;49mcrop_to_aspect_ratio)\n\u001b[0;32m    221\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mprefetch(tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mAUTOTUNE)\n\u001b[0;32m    222\u001b[0m \u001b[39mif\u001b[39;00m batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\ohguk\\py3.8\\lib\\site-packages\\keras\\utils\\image_dataset.py:250\u001b[0m, in \u001b[0;36mpaths_and_labels_to_dataset\u001b[1;34m(image_paths, image_size, num_channels, labels, label_mode, num_classes, interpolation, crop_to_aspect_ratio)\u001b[0m\n\u001b[0;32m    248\u001b[0m path_ds \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices(image_paths)\n\u001b[0;32m    249\u001b[0m args \u001b[39m=\u001b[39m (image_size, num_channels, interpolation, crop_to_aspect_ratio)\n\u001b[1;32m--> 250\u001b[0m img_ds \u001b[39m=\u001b[39m path_ds\u001b[39m.\u001b[39;49mmap(\n\u001b[0;32m    251\u001b[0m     \u001b[39mlambda\u001b[39;49;00m x: load_image(x, \u001b[39m*\u001b[39;49margs), num_parallel_calls\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mAUTOTUNE)\n\u001b[0;32m    252\u001b[0m \u001b[39mif\u001b[39;00m label_mode:\n\u001b[0;32m    253\u001b[0m   label_ds \u001b[39m=\u001b[39m dataset_utils\u001b[39m.\u001b[39mlabels_to_dataset(labels, label_mode, num_classes)\n",
      "File \u001b[1;32md:\\ohguk\\py3.8\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2050\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[1;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[0;32m   2048\u001b[0m   \u001b[39mreturn\u001b[39;00m MapDataset(\u001b[39mself\u001b[39m, map_func, preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, name\u001b[39m=\u001b[39mname)\n\u001b[0;32m   2049\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2050\u001b[0m   \u001b[39mreturn\u001b[39;00m ParallelMapDataset(\n\u001b[0;32m   2051\u001b[0m       \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   2052\u001b[0m       map_func,\n\u001b[0;32m   2053\u001b[0m       num_parallel_calls,\n\u001b[0;32m   2054\u001b[0m       deterministic,\n\u001b[0;32m   2055\u001b[0m       preserve_cardinality\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   2056\u001b[0m       name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32md:\\ohguk\\py3.8\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:5284\u001b[0m, in \u001b[0;36mParallelMapDataset.__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[0;32m   5282\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_dataset \u001b[39m=\u001b[39m input_dataset\n\u001b[0;32m   5283\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_inter_op_parallelism \u001b[39m=\u001b[39m use_inter_op_parallelism\n\u001b[1;32m-> 5284\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func \u001b[39m=\u001b[39m structured_function\u001b[39m.\u001b[39;49mStructuredFunctionWrapper(\n\u001b[0;32m   5285\u001b[0m     map_func,\n\u001b[0;32m   5286\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transformation_name(),\n\u001b[0;32m   5287\u001b[0m     dataset\u001b[39m=\u001b[39;49minput_dataset,\n\u001b[0;32m   5288\u001b[0m     use_legacy_function\u001b[39m=\u001b[39;49muse_legacy_function)\n\u001b[0;32m   5289\u001b[0m \u001b[39mif\u001b[39;00m deterministic \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   5290\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deterministic \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32md:\\ohguk\\py3.8\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:271\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m       warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    265\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    266\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    267\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    268\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    269\u001b[0m     fn_factory \u001b[39m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[1;32m--> 271\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function \u001b[39m=\u001b[39m fn_factory()\n\u001b[0;32m    272\u001b[0m \u001b[39m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[0;32m    273\u001b[0m add_to_graph \u001b[39m&\u001b[39m\u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[1;32md:\\ohguk\\py3.8\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2567\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2558\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_concrete_function\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   2559\u001b[0m   \u001b[39m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001b[39;00m\n\u001b[0;32m   2560\u001b[0m \n\u001b[0;32m   2561\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2565\u001b[0m \u001b[39m       or `tf.Tensor` or `tf.TensorSpec`.\u001b[39;00m\n\u001b[0;32m   2566\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2567\u001b[0m   graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_concrete_function_garbage_collected(\n\u001b[0;32m   2568\u001b[0m       \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   2569\u001b[0m   graph_function\u001b[39m.\u001b[39m_garbage_collector\u001b[39m.\u001b[39mrelease()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   2570\u001b[0m   \u001b[39mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32md:\\ohguk\\py3.8\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2533\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2531\u001b[0m   args, kwargs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2532\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m-> 2533\u001b[0m   graph_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n\u001b[0;32m   2534\u001b[0m   seen_names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m   2535\u001b[0m   captured \u001b[39m=\u001b[39m object_identity\u001b[39m.\u001b[39mObjectIdentitySet(\n\u001b[0;32m   2536\u001b[0m       graph_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39minternal_captures)\n",
      "File \u001b[1;32md:\\ohguk\\py3.8\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2711\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2708\u001b[0m   cache_key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_cache\u001b[39m.\u001b[39mgeneralize(cache_key)\n\u001b[0;32m   2709\u001b[0m   (args, kwargs) \u001b[39m=\u001b[39m cache_key\u001b[39m.\u001b[39m_placeholder_value()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 2711\u001b[0m graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_graph_function(args, kwargs)\n\u001b[0;32m   2712\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_cache\u001b[39m.\u001b[39madd(cache_key, cache_key_deletion_observer,\n\u001b[0;32m   2713\u001b[0m                          graph_function)\n\u001b[0;32m   2715\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
      "File \u001b[1;32md:\\ohguk\\py3.8\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2627\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2622\u001b[0m missing_arg_names \u001b[39m=\u001b[39m [\n\u001b[0;32m   2623\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (arg, i) \u001b[39mfor\u001b[39;00m i, arg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(missing_arg_names)\n\u001b[0;32m   2624\u001b[0m ]\n\u001b[0;32m   2625\u001b[0m arg_names \u001b[39m=\u001b[39m base_arg_names \u001b[39m+\u001b[39m missing_arg_names\n\u001b[0;32m   2626\u001b[0m graph_function \u001b[39m=\u001b[39m ConcreteFunction(\n\u001b[1;32m-> 2627\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[0;32m   2628\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[0;32m   2629\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[0;32m   2630\u001b[0m         args,\n\u001b[0;32m   2631\u001b[0m         kwargs,\n\u001b[0;32m   2632\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_signature,\n\u001b[0;32m   2633\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[0;32m   2634\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[0;32m   2635\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[0;32m   2636\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value),\n\u001b[0;32m   2637\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[0;32m   2638\u001b[0m     spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[0;32m   2639\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m   2640\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m   2641\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m   2642\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m   2643\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   2644\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32md:\\ohguk\\py3.8\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1141\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1138\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1139\u001b[0m   _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1141\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39;49mfunc_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunc_kwargs)\n\u001b[0;32m   1143\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m func_outputs \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mmap_structure(\n\u001b[0;32m   1146\u001b[0m     convert, func_outputs, expand_composites\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\ohguk\\py3.8\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:248\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[39m@eager_function\u001b[39m\u001b[39m.\u001b[39mdefun_with_attributes(\n\u001b[0;32m    243\u001b[0m     input_signature\u001b[39m=\u001b[39mstructure\u001b[39m.\u001b[39mget_flat_tensor_specs(\n\u001b[0;32m    244\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_structure),\n\u001b[0;32m    245\u001b[0m     autograph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    246\u001b[0m     attributes\u001b[39m=\u001b[39mdefun_kwargs)\n\u001b[0;32m    247\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_fn\u001b[39m(\u001b[39m*\u001b[39margs):  \u001b[39m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[1;32m--> 248\u001b[0m   ret \u001b[39m=\u001b[39m wrapper_helper(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    249\u001b[0m   ret \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mto_tensor_list(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_structure, ret)\n\u001b[0;32m    250\u001b[0m   \u001b[39mreturn\u001b[39;00m [ops\u001b[39m.\u001b[39mconvert_to_tensor(t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m ret]\n",
      "File \u001b[1;32md:\\ohguk\\py3.8\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:177\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _should_unpack(nested_args):\n\u001b[0;32m    176\u001b[0m   nested_args \u001b[39m=\u001b[39m (nested_args,)\n\u001b[1;32m--> 177\u001b[0m ret \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39;49mtf_convert(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func, ag_ctx)(\u001b[39m*\u001b[39;49mnested_args)\n\u001b[0;32m    178\u001b[0m \u001b[39mif\u001b[39;00m _should_pack(ret):\n\u001b[0;32m    179\u001b[0m   ret \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(ret)\n",
      "File \u001b[1;32md:\\ohguk\\py3.8\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:689\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    688\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[1;32m--> 689\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39;49moptions)\n\u001b[0;32m    690\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    691\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[1;32md:\\ohguk\\py3.8\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    374\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    376\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39muser_requested \u001b[39mand\u001b[39;00m conversion\u001b[39m.\u001b[39mis_allowlisted(f):\n\u001b[1;32m--> 377\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    379\u001b[0m \u001b[39m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[39m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[39m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[39m# things like builtins.\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[1;32md:\\ohguk\\py3.8\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:458\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    455\u001b[0m   \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39mcall(args, kwargs)\n\u001b[0;32m    457\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 458\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    459\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[1;32md:\\ohguk\\py3.8\\lib\\site-packages\\keras\\utils\\image_dataset.py:251\u001b[0m, in \u001b[0;36mpaths_and_labels_to_dataset.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    248\u001b[0m path_ds \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices(image_paths)\n\u001b[0;32m    249\u001b[0m args \u001b[39m=\u001b[39m (image_size, num_channels, interpolation, crop_to_aspect_ratio)\n\u001b[0;32m    250\u001b[0m img_ds \u001b[39m=\u001b[39m path_ds\u001b[39m.\u001b[39mmap(\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mlambda\u001b[39;00m x: load_image(x, \u001b[39m*\u001b[39;49margs), num_parallel_calls\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mAUTOTUNE)\n\u001b[0;32m    252\u001b[0m \u001b[39mif\u001b[39;00m label_mode:\n\u001b[0;32m    253\u001b[0m   label_ds \u001b[39m=\u001b[39m dataset_utils\u001b[39m.\u001b[39mlabels_to_dataset(labels, label_mode, num_classes)\n",
      "File \u001b[1;32md:\\ohguk\\py3.8\\lib\\site-packages\\keras\\utils\\image_dataset.py:267\u001b[0m, in \u001b[0;36mload_image\u001b[1;34m(path, image_size, num_channels, interpolation, crop_to_aspect_ratio)\u001b[0m\n\u001b[0;32m    265\u001b[0m   img \u001b[39m=\u001b[39m image_utils\u001b[39m.\u001b[39msmart_resize(img, image_size, interpolation\u001b[39m=\u001b[39minterpolation)\n\u001b[0;32m    266\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 267\u001b[0m   img \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mimage\u001b[39m.\u001b[39;49mresize(img, image_size, method\u001b[39m=\u001b[39;49minterpolation)\n\u001b[0;32m    268\u001b[0m img\u001b[39m.\u001b[39mset_shape((image_size[\u001b[39m0\u001b[39m], image_size[\u001b[39m1\u001b[39m], num_channels))\n\u001b[0;32m    269\u001b[0m \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32md:\\ohguk\\py3.8\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\ohguk\\py3.8\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py:1452\u001b[0m, in \u001b[0;36m_resize_images_common\u001b[1;34m(images, resizer_fn, size, preserve_aspect_ratio, name, skip_resize_if_same)\u001b[0m\n\u001b[0;32m   1450\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39msize\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m must be a 1-D int32 Tensor\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   1451\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m size\u001b[39m.\u001b[39mget_shape()\u001b[39m.\u001b[39mis_compatible_with([\u001b[39m2\u001b[39m]):\n\u001b[1;32m-> 1452\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39msize\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m must be a 1-D Tensor of 2 elements: \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1453\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39mnew_height, new_width\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   1455\u001b[0m \u001b[39mif\u001b[39;00m preserve_aspect_ratio:\n\u001b[0;32m   1456\u001b[0m   \u001b[39m# Get the current shapes of the image, even if dynamic.\u001b[39;00m\n\u001b[0;32m   1457\u001b[0m   _, current_height, current_width, _ \u001b[39m=\u001b[39m _ImageDimensions(images, rank\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: 'size' must be a 1-D Tensor of 2 elements: new_height, new_width"
     ]
    }
   ],
   "source": [
    "# Code to compile and train the model\n",
    "train_ds, val_ds = solution_model()\n",
    "model.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(train_ds, epochs = 20)\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that you'll need to save your model as a .h5 like this.\n",
    "# When you press the Submit and Test button, your saved .h5 model will\n",
    "# be sent to the testing infrastructure for scoring\n",
    "# and the score will be returned to you.\n",
    "if __name__ == '__main__':\n",
    "   model = solution_model()\n",
    "   model.save(\"mymodel.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "54409eae9e1c57883ccd4f88707ee2e01c9055555e7b97a265283b72fed450ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
