{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
    "# Please note that the weight of the grade for the question is relative\n",
    "# to its difficulty. So your Category 1 question will score significantly\n",
    "# less than your Category 5 question.\n",
    "#\n",
    "# Don't use lambda layers in your model.\n",
    "# You do not need them to solve the question.\n",
    "# Lambda layers are not supported by the grading infrastructure.\n",
    "#\n",
    "# You must use the Submit and Test button to submit your model\n",
    "# at least once in this category before you finally submit your exam,\n",
    "# otherwise you will score zero for this category.\n",
    "# ======================================================================\n",
    "#\n",
    "# NLP QUESTION\n",
    "#\n",
    "# Build and train a classifier for the sarcasm dataset.\n",
    "# The classifier should have a final layer with 1 neuron activated by sigmoid as shown.\n",
    "# It will be tested against a number of sentences that the network hasn't previously seen\n",
    "# and you will be scored on whether sarcasm was correctly detected in those sentences.\n",
    " \n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import urllib\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "#import matplotlib.pyplot as plt\n",
    "#from scipy.stats import linregress\n",
    " \n",
    "def solution_model():\n",
    "   url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
    "   urllib.request.urlretrieve(url, 'sarcasm.json')\n",
    " \n",
    "   # DO NOT CHANGE THIS CODE OR THE TESTS MAY NOT WORK\n",
    "   vocab_size = 1000\n",
    "   embedding_dim = 16\n",
    "   max_length = 120\n",
    "   trunc_type='post'\n",
    "   padding_type='post'\n",
    "   oov_tok = \"<OOV>\"\n",
    "   training_size = 20000\n",
    " \n",
    "   lstm_dim = 32\n",
    "   dense_dim = 24\n",
    " \n",
    "   sentences = []\n",
    "   labels = []\n",
    "   # YOUR CODE HERE\n",
    "   with open(\"./sarcasm.json\", 'r') as f:\n",
    "       datastore = json.load(f)\n",
    " \n",
    "   for item in datastore:\n",
    "       sentences.append(item['headline'])\n",
    "       labels.append(item['is_sarcastic'])\n",
    " \n",
    "   # Split the sentences\n",
    "   training_sentences = sentences[0:training_size]\n",
    "   testing_sentences = sentences[training_size:]\n",
    " \n",
    "   # Split the labels\n",
    "   training_labels = labels[0:training_size]\n",
    "   testing_labels = labels[training_size:]\n",
    " \n",
    "   tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    " \n",
    "   # Generate the word index dictionary\n",
    "   tokenizer.fit_on_texts(training_sentences)\n",
    "   word_index = tokenizer.word_index\n",
    " \n",
    "   # Generate and pad the training sequences\n",
    "   training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "   training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    " \n",
    "   # Generate and pad the testing sequences\n",
    "   testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "   testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    " \n",
    "   # Convert the labels lists into numpy arrays\n",
    "   training_labels = np.array(training_labels)\n",
    "   testing_labels = np.array(testing_labels)\n",
    " \n",
    "   model = tf.keras.Sequential([\n",
    "       tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "       tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_dim)),\n",
    "       tf.keras.layers.Dense(dense_dim, activation='relu'),\n",
    "       tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "   ])\n",
    " \n",
    "   model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    " \n",
    "   model.fit(training_padded, training_labels, epochs=40, validation_data=(testing_padded, testing_labels),\n",
    "             verbose=2)\n",
    " \n",
    "   return model\n",
    " \n",
    " \n",
    "# Note that you'll need to save your model as a .h5 like this.\n",
    "# When you press the Submit and Test button, your saved .h5 model will\n",
    "# be sent to the testing infrastructure for scoring\n",
    "# and the score will be returned to you.\n",
    "if __name__ == '__main__':\n",
    "   model = solution_model()\n",
    "   model.save(\"mymodel.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "54409eae9e1c57883ccd4f88707ee2e01c9055555e7b97a265283b72fed450ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
