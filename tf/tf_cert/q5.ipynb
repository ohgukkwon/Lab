{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "81QWXYXxojcn"
      },
      "outputs": [],
      "source": [
        "# Make sure that the model architecture and input, output shapes match our\n",
        "# requirements by printing model.summary() and reviewing its output.\n",
        "#\n",
        "# HINT: If you follow all the rules mentioned above and throughout this\n",
        "# question while training your neural network, there is a possibility that a\n",
        "# validation MAE of approximately 0.055 or less on the normalized validation\n",
        "# dataset may fetch you top marks.\n",
        "import urllib\n",
        "import zipfile\n",
        " \n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrJ0OW9Uoqcz",
        "outputId": "9ad5dcda-ff3f-4b1c-9c9b-34382e872578"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "file exist\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "file = pathlib.Path(\"household_power.zip\")\n",
        "if file.exists ():\n",
        "  print(\"file exist\")    \n",
        "else:\n",
        "  wget.download('https://storage.googleapis.com/download.tensorflow.org/data/certificate/household_power.zip')\n",
        "  with zipfile.ZipFile('household_power.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-kXiESZ2qD4n"
      },
      "outputs": [],
      "source": [
        "# This function normalizes the dataset using min max scaling.\n",
        "# DO NOT CHANGE THIS CODE\n",
        "def normalize_series(data, min, max):\n",
        "   data = data - min\n",
        "   data = data / max\n",
        "   return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sooT4ZzeqGg-"
      },
      "outputs": [],
      "source": [
        "# This function is used to map the time series dataset into windows of\n",
        "# features and respective targets, to prepare it for training and\n",
        "# validation. First element of the first window will be the first element of\n",
        "# the dataset. Consecutive windows are constructed by shifting\n",
        "# the starting position of the first window forward, one at a time (indicated\n",
        "# by shift=1). For a window of n_past number of observations of all the time\n",
        "# indexed variables in the dataset, the target for the window\n",
        "# is the next n_future number of observations of these variables, after the\n",
        "# end of the window.\n",
        " \n",
        "# DO NOT CHANGE THIS CODE\n",
        "def windowed_dataset(series, batch_size, n_past=24, n_future=24, shift=1):\n",
        "   ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "   ds = ds.window(size=n_past + n_future, shift=shift, drop_remainder=True)\n",
        "   ds = ds.flat_map(lambda w: w.batch(n_past + n_future))\n",
        "   ds = ds.map(lambda w: (w[:n_past], w[n_past:]))\n",
        "   return ds.batch(batch_size).prefetch(1)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ASIx3vFoqI5S"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(43200, 7)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "cannot reshape array of size 302400 into shape (32,24,7)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mC:\\Users\\O02A2~1.KWO\\AppData\\Local\\Temp/ipykernel_15600/3878254556.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_FEATURES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_PAST\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m \u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_FEATURES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_PAST\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msolution_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'household_power_consumption.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;31m#print(train_set.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Users\\O02A2~1.KWO\\AppData\\Local\\Temp/ipykernel_15600/3878254556.py\u001b[0m in \u001b[0;36msolution_model\u001b[1;34m(csv_file)\u001b[0m\n\u001b[0;32m     53\u001b[0m                                 shift=SHIFT)\n\u001b[0;32m     54\u001b[0m    \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m    \u001b[0min_shp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_PAST\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_FEATURES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_FEATURES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_PAST\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 302400 into shape (32,24,7)"
          ]
        }
      ],
      "source": [
        "# This function loads the data from CSV file, normalizes the data and\n",
        "# splits the dataset into train and validation data. It also uses\n",
        "# windowed_dataset() to split the data into windows of observations and\n",
        "# targets. Finally it defines, compiles and trains a neural network. This\n",
        "# function returns the final trained model.\n",
        " \n",
        "# COMPLETE THE CODE IN THIS FUNCTION\n",
        "def solution_model(csv_file):\n",
        "   # Downloads and extracts the dataset to the directory that\n",
        "   # contains this file.\n",
        "   #download_and_extract_data()\n",
        "   # Reads the dataset from the CSV.\n",
        "   df = pd.read_csv(csv_file, sep=',', infer_datetime_format=True, index_col='datetime', header=0)\n",
        " \n",
        "   # Number of features in the dataset. We use all features as predictors to\n",
        "   # predict all features at future time steps.\n",
        "   N_FEATURES = len(df.columns) # DO NOT CHANGE THIS\n",
        " \n",
        "   # Normalizes the data\n",
        "   data = df.values\n",
        "   data = normalize_series(data, data.min(axis=0), data.max(axis=0))\n",
        " \n",
        "   # Splits the data into training and validation sets.\n",
        "   SPLIT_TIME = int(len(data) * 0.5) # DO NOT CHANGE THIS\n",
        "   x_train = data[:SPLIT_TIME]\n",
        "   x_valid = data[SPLIT_TIME:]\n",
        " \n",
        "   # DO NOT CHANGE THIS CODE\n",
        "   tf.keras.backend.clear_session()\n",
        "   tf.random.set_seed(42)\n",
        "\n",
        "   BATCH_SIZE = 32  # ADVISED NOT TO CHANGE THIS\n",
        " \n",
        "   # DO NOT CHANGE N_PAST, N_FUTURE, SHIFT. The tests will fail to run\n",
        "   # on the server.\n",
        "   # Number of past time steps based on which future observations should be\n",
        "   # predicted\n",
        "   N_PAST = 24  # DO NOT CHANGE THIS\n",
        " \n",
        "   # Number of future time steps which are to be predicted.\n",
        "   N_FUTURE = 24  # DO NOT CHANGE THIS\n",
        " \n",
        "   # By how many positions the window slides to create a new window\n",
        "   # of observations.\n",
        "   SHIFT = 1  # DO NOT CHANGE THIS\n",
        " \n",
        "   # Code to create windowed train and validation datasets.\n",
        "   train_set = windowed_dataset(series=x_train, batch_size=BATCH_SIZE,\n",
        "                                n_past=N_PAST, n_future=N_FUTURE,\n",
        "                                shift=SHIFT)\n",
        "   valid_set = windowed_dataset(series=x_valid, batch_size=BATCH_SIZE,\n",
        "                                n_past=N_PAST, n_future=N_FUTURE,\n",
        "                                shift=SHIFT)\n",
        "   print(x_train.shape)\n",
        "   #in_shp = x_train.reshape(BATCH_SIZE, N_PAST, N_FEATURES)\n",
        "   return train_set, valid_set, N_FEATURES, BATCH_SIZE, N_PAST\n",
        "\n",
        "train_set, valid_set, N_FEATURES, BATCH_SIZE, N_PAST = solution_model('household_power_consumption.csv')\n",
        "#print(train_set.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "LDHT1vd5n1Jg",
        "outputId": "e8ba12d7-16b8-4af9-d548-21a06627af9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 24, 7)             420       \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 24, 14)            112       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 24, 7)             105       \n",
            "=================================================================\n",
            "Total params: 637\n",
            "Trainable params: 637\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python38\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1349/1349 [==============================] - 34s 22ms/step - loss: 0.0783 - mae: 0.1443 - val_loss: 0.0836 - val_mae: 0.1486\n",
            "Epoch 2/20\n",
            "1349/1349 [==============================] - 32s 24ms/step - loss: 0.0783 - mae: 0.1443 - val_loss: 0.0836 - val_mae: 0.1486ss: 0.0767 - mae: 0.1 - ETA: 0s - loss: 0.0\n",
            "Epoch 3/20\n",
            "1349/1349 [==============================] - 35s 26ms/step - loss: 0.0783 - mae: 0.1443 - val_loss: 0.0836 - val_mae: 0.1486ss:  - ETA: 1s - loss: 0.0768 - mae: 0. - ETA: 1s - loss: 0.0767 - mae: 0.142 - ETA: 1s - loss: 0.0766 - - ETA: 0s - loss: 0.0778 - \n",
            "Epoch 4/20\n",
            "1349/1349 [==============================] - 38s 28ms/step - loss: 0.0783 - mae: 0.1443 - val_loss: 0.0836 - val_mae: 0.1486- lo - ETA: 14 - ETA - ETA: 3s - loss: 0.0766 - ETA: 0s - loss: 0.0777 - mae:  - ETA: 0s - loss: 0.0780 - mae: 0.1\n",
            "Epoch 5/20\n",
            "1349/1349 [==============================] - 37s 27ms/step - loss: 0.0783 - mae: 0.1443 - val_loss: 0.0836 - val_mae: 0.1486loss: 0.0781 - mae:\n",
            "Epoch 6/20\n",
            "1349/1349 [==============================] - 37s 27ms/step - loss: 0.0783 - mae: 0.1443 - val_loss: 0.0836 - val_mae: 0.1486\n",
            "Epoch 7/20\n",
            "1349/1349 [==============================] - 30s 23ms/step - loss: 0.0783 - mae: 0.1443 - val_loss: 0.0836 - val_mae: 0.1486\n",
            "Epoch 8/20\n",
            "1349/1349 [==============================] - 31s 23ms/step - loss: 0.0783 - mae: 0.1443 - val_loss: 0.0836 - val_mae: 0.1485\n",
            "Epoch 9/20\n",
            "1349/1349 [==============================] - 40s 30ms/step - loss: 0.0783 - mae: 0.1443 - val_loss: 0.0836 - val_mae: 0.1485\n",
            "Epoch 10/20\n",
            "1349/1349 [==============================] - 35s 26ms/step - loss: 0.0783 - mae: 0.1443 - val_loss: 0.0836 - val_mae: 0.14850.1 - ETA: 6s - loss: 0.07\n",
            "Epoch 11/20\n",
            "1349/1349 [==============================] - 30s 23ms/step - loss: 0.0783 - mae: 0.1443 - val_loss: 0.0836 - val_mae: 0.1485\n",
            "Epoch 12/20\n",
            "1349/1349 [==============================] - 31s 23ms/step - loss: 0.0783 - mae: 0.1443 - val_loss: 0.0836 - val_mae: 0.1485s: \n",
            "Epoch 13/20\n",
            "1349/1349 [==============================] - 31s 23ms/step - loss: 0.0783 - mae: 0.1443 - val_loss: 0.0836 - val_mae: 0.1485\n",
            "Epoch 14/20\n",
            "1349/1349 [==============================] - 35s 26ms/step - loss: 0.0783 - mae: 0.1443 - val_loss: 0.0836 - val_mae: 0.1485\n",
            "Epoch 15/20\n",
            "1349/1349 [==============================] - 31s 23ms/step - loss: 0.0783 - mae: 0.1443 - val_loss: 0.0835 - val_mae: 0.1485\n",
            "Epoch 16/20\n",
            "1349/1349 [==============================] - 33s 24ms/step - loss: 0.0783 - mae: 0.1443 - val_loss: 0.0835 - val_mae: 0.148585 - ETA: 13s - ETA: 9s - loss: 0.0 - ETA: 8s - loss: 0.0730 - - ETA: 7s - lo - ETA: 3s - loss: 0.0769 - mae:  - ETA:  - ETA: 1s - loss:  - ETA: 0s - loss: 0.0780 - mae: 0.\n",
            "Epoch 17/20\n",
            "1349/1349 [==============================] - 33s 25ms/step - loss: 0.0783 - mae: 0.1442 - val_loss: 0.0835 - val_mae: 0.1485- l - ETA: 0s - loss: 0.0780 - mae: 0.14\n",
            "Epoch 18/20\n",
            "1349/1349 [==============================] - 31s 23ms/step - loss: 0.0782 - mae: 0.1442 - val_loss: 0.0835 - val_mae: 0.1485\n",
            "Epoch 19/20\n",
            "1349/1349 [==============================] - 31s 23ms/step - loss: 0.0782 - mae: 0.1442 - val_loss: 0.0835 - val_mae: 0.1485\n",
            "Epoch 20/20\n",
            "1349/1349 [==============================] - 31s 23ms/step - loss: 0.0782 - mae: 0.1442 - val_loss: 0.0835 - val_mae: 0.1485\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1d944309940>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Code to define your model.\n",
        "# Code to define your model.\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.LSTM(7, return_sequences=True, input_shape=(N_PAST, N_FEATURES)),\n",
        "    tf.keras.layers.Dense(14, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(N_FEATURES)])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "optimizer =  tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n",
        "model.compile(\n",
        "    loss=\"mse\",\n",
        "    optimizer=optimizer,\n",
        "    metrics=[\"mae\"]\n",
        ")\n",
        "model.fit(train_set, validation_data=valid_set,validation_steps=100, epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhgRCk-EqciP"
      },
      "outputs": [],
      "source": [
        "model.save(\"mymodelq5.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oueHa0HTpHOq"
      },
      "outputs": [],
      "source": [
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        " \n",
        "'''if __name__ == '__main__':\n",
        "   model = solution_model()\n",
        "   model.save(\"mymodel.h5\")\n",
        " '''\n",
        " \n",
        "# THIS CODE IS USED IN THE TESTER FOR FORECASTING. IF YOU WANT TO TEST YOUR MODEL\n",
        "# BEFORE UPLOADING YOU CAN DO IT WITH THIS\n",
        "#def mae(y_true, y_pred):\n",
        "#    return np.mean(abs(y_true.ravel() - y_pred.ravel()))\n",
        "#\n",
        "#\n",
        "#def model_forecast(model, series, window_size, batch_size):\n",
        "#    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "#    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
        "#    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
        "#    ds = ds.batch(batch_size, drop_remainder=True).prefetch(1)\n",
        "#    forecast = model.predict(ds)\n",
        "#    return forecast\n",
        "#\n",
        " \n",
        "# PASS THE NORMALIZED data IN THE FOLLOWING CODE\n",
        " \n",
        "#rnn_forecast = model_forecast(model, data, N_PAST, BATCH_SIZE)\n",
        "#rnn_forecast = rnn_forecast[SPLIT_TIME - N_PAST:-1, 0, :]\n",
        " \n",
        "#x_valid = x_valid[:rnn_forecast.shape[0]]\n",
        "#result = mae(x_valid, rnn_forecast)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "# ADD YOUR LAYERS HERE.\n",
        " \n",
        "# If you don't follow the instructions in the following comments,\n",
        "# tests will fail to grade your code:\n",
        "# The input layer of your model must have an input shape of:\n",
        "# (BATCH_SIZE, N_PAST = 24, N_FEATURES = 7)\n",
        "# The model must have an output shape of:\n",
        "# (BATCH_SIZE, N_FUTURE = 24, N_FEATURES = 7).\n",
        "# Make sure that there are N_FEATURES = 7 neurons in the final dense\n",
        "# layer since the model predicts 7 features.\n",
        " \n",
        "# HINT: Bidirectional LSTMs may help boost your score. This is only a\n",
        "# suggestion.\n",
        " \n",
        "# WARNING: After submitting the trained model for scoring, if you are\n",
        "# receiving a score of 0 or an error, please recheck the input and\n",
        "# output shapes of the model to see if it exactly matches our requirements.\n",
        "# The grading infrastructure is very strict about the shape requirements.\n",
        "# Most common issues occur when the shapes are not matching our\n",
        "# expectations.\n",
        "#\n",
        "# TIP: You can print the output of model.summary() to review the model\n",
        "# architecture, input and output shapes of each layer.\n",
        "# If you have made sure that you have matched the shape requirements\n",
        "# and all the other instructions we have laid down, and still\n",
        "# receive a bad score, you must work on improving your model.\n",
        " \n",
        "# WARNING: If you are using the GRU layer, it is advised not to use the\n",
        "# recurrent_dropout argument (you can alternatively set it to 0),\n",
        "# since it has not been implemented in the cuDNN kernel and may\n",
        "# result in much longer training times."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3 (tags/v3.8.3:6f8c832, May 13 2020, 22:37:02) [MSC v.1924 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
